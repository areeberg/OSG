{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spektral.transforms import AdjToSpTensor, LayerPreprocess\n",
    "\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import Dataset, DisjointLoader, Graph\n",
    "from spektral.layers import GCSConv, GlobalAvgPool\n",
    "from spektral.datasets import TUDataset\n",
    "from spektral.models import GeneralGNN\n",
    "from spektral.transforms.normalize_adj import NormalizeAdj\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy, binary_accuracy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Customdataset(Dataset):\n",
    "    def __init__(self, graph, labels,  **kwargs):\n",
    "        self.graph = graph\n",
    "        self.labels = labels\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        output = []\n",
    "        for gr, lb in zip(self.graph, self.labels):\n",
    "            num_nodes = len(list(gr.nodes))\n",
    "            list_edges = list(gr.edges)\n",
    "            x = np.zeros((num_nodes, 982))\n",
    "            auxvar = 0\n",
    "            initial_value = list_edges[0][0]\n",
    "            for edge in list_edges:\n",
    "                control = edge[0]\n",
    "                if control == initial_value:\n",
    "                    x[auxvar][edge[1]] += 1\n",
    "                else:\n",
    "                    initial_value = control\n",
    "                    auxvar += 1\n",
    "                    x[auxvar][edge[1]] += 1\n",
    "\n",
    "            A = nx.to_numpy_matrix(gr)\n",
    "            A = sp.csr_matrix(A)\n",
    "            X = np.asarray(x)\n",
    "\n",
    "            E = np.asarray(list(gr.edges))\n",
    "            E = sp.csr_matrix(E)\n",
    "\n",
    "            if lb == 1:\n",
    "                Y = np.array([1, 0])\n",
    "            else:\n",
    "                Y = np.array([0, 1])\n",
    "            output.append(\n",
    "                Graph(x=X,\n",
    "                      a=A,\n",
    "                      e=None,\n",
    "                      y=Y))\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Customdataset(graph=listgraph, labels=listlabels, transforms=NormalizeAdj())\n",
    "idx = np.random.permutation(len(dataset))\n",
    "split_va = int(0.9*len(dataset))\n",
    "idx_tr, idx_va = np.split(idx, [split_va])\n",
    "data_tr = dataset[idx_tr]\n",
    "data_va = dataset[idx_va]\n",
    "\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=32, epochs=25)\n",
    "loader_va = DisjointLoader(data_va, batch_size=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 10  # Number of training epochs\n",
    "es_patience = 10  # Patience for early stopping\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "model = GeneralGNN(dataset.n_labels, activation=\"softmax\")\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit model\n",
    "\n",
    "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
    "def train_step(inputs, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target, predictions) + sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    acc = tf.reduce_mean(categorical_accuracy(target, predictions))\n",
    "    return loss, acc\n",
    "\n",
    "def evaluate(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        outs = (\n",
    "            loss_fn(target, pred),\n",
    "            tf.reduce_mean(binary_accuracy(target, pred)),\n",
    "            len(target),  # Keep track of batch size\n",
    "        )\n",
    "        output.append(outs)\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "def inference(loader):\n",
    "    output = []\n",
    "    step = 0\n",
    "    while step < loader.steps_per_epoch:\n",
    "        step += 1\n",
    "        inputs, target = loader.__next__()\n",
    "        pred = model(inputs, training=False)\n",
    "        output.append(pred)\n",
    "\n",
    "        if step == loader.steps_per_epoch:\n",
    "            output = np.array(output)\n",
    "            return np.average(output[:, :-1], 0, weights=output[:, -1])\n",
    "\n",
    "\n",
    "epoch = step = 0\n",
    "best_val_loss = np.inf\n",
    "best_weights = None\n",
    "patience = es_patience\n",
    "results = []\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train loop\n",
    "\n",
    "for batch in loader_tr:\n",
    "    step += 1\n",
    "    loss, acc = train_step(*batch)\n",
    "    results.append((loss, acc))\n",
    "    if step == loader_tr.steps_per_epoch:\n",
    "        step = 0\n",
    "        epoch += 1\n",
    "\n",
    "        # Compute validation loss and accuracy\n",
    "        val_loss, val_acc = evaluate(loader_va)\n",
    "        print(\n",
    "            \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val loss: {:.3f} - Val acc: {:.3f}\".format(\n",
    "                epoch, *np.mean(results, 0), val_loss, val_acc\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Check if loss improved for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = es_patience\n",
    "            print(\"New best val_loss {:.3f}\".format(val_loss))\n",
    "            best_weights = model.get_weights()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping (best val_loss: {})\".format(best_val_loss))\n",
    "                break\n",
    "        results = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}