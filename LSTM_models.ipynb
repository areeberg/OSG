{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, MaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "Implementation of the attention layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
    "                               initializer='zeros', trainable=True)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "Bi-LSTM implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LstmModel:\n",
    "    def __init__(self, max_words, embedding_vector_len, max_len):\n",
    "        self.max_words = max_words\n",
    "        self.embedding_vector_length = embedding_vector_len\n",
    "        self.max_len = max_len\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Embedding(self.max_words, self.embedding_vector_length, input_length=self.max_len))\n",
    "        self.model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Bidirectional(LSTM(200,return_sequences=True))) #added bidirectional\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(attention())\n",
    "        self.model.add(Dense(2, activation='softmax'))\n",
    "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        return self.model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = LstmModel(max_len=3000,max_words=3000,embedding_vector_len=500).get_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=0,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    ")\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model.fit(np.array(X_train), np.array(Y_train), epochs=30, batch_size=32,validation_data=(np.array(X_test),np.array(Y_test)), callbacks=[tensorboard_callback,early_stop_callback])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "C-LSTM Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LstmModelC:\n",
    "    def __init__(self, max_words, embedding_vector_len, max_len):\n",
    "        self.max_words = max_words\n",
    "        self.embedding_vector_length = embedding_vector_len\n",
    "        self.max_len = max_len\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model = Sequential()\n",
    "\n",
    "        self.model.add(Embedding(self.max_words, self.embedding_vector_length, input_length=self.max_len))\n",
    "        self.model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        self.model.add(MaxPooling1D(pool_size=2))\n",
    "        self.model.add(LSTM(200))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(2, activation='softmax'))\n",
    "        self.model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        return self.model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelC = LstmModelC(max_len=3000,max_words=3000,embedding_vector_len=500).get_model()\n",
    "modelC.summary()\n",
    "modelC.fit(np.array(X_train), np.array(Y_train), epochs=30, batch_size=32,validation_data=(np.array(X_test),np.array(Y_test)), callbacks=[tensorboard_callback,early_stop_callback])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}